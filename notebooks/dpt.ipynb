{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef6bfca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/nazirnayal/DATA/projects/attention_long_tail/paying-attention-to-the-long-tail'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0f0142",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56805a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from collections import namedtuple\n",
    "from models.dpt.models import DPTSegmentationModel\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e991758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74214265",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config('configs/segmentation/dpt_config.yaml')\n",
    "config = namedtuple('config', config.keys())(*config.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "075021f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DPTSegmentationModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3869d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "H = 384\n",
    "W = 384\n",
    "C = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b3e33e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((N, C, H, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2dc85ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "120796f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 12, 224, 224])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23e2e328",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─Module: 1                                   []                        --\n",
      "|    └─VisionTransformer: 2                   []                        --\n",
      "|    |    └─Dropout: 3-1                      [-1, 197, 768]            --\n",
      "|    |    └─LayerNorm: 3-2                    [-1, 197, 768]            1,536\n",
      "|    └─Sequential: 2                          []                        --\n",
      "|    |    └─ProjectReadout: 3-3               [-1, 196, 768]            1,180,416\n",
      "|    |    └─Transpose: 3-4                    [-1, 768, 196]            --\n",
      "|    └─Sequential: 2                          []                        --\n",
      "|    |    └─ProjectReadout: 3-5               [-1, 196, 768]            1,180,416\n",
      "|    |    └─Transpose: 3-6                    [-1, 768, 196]            --\n",
      "|    └─Sequential: 2                          []                        --\n",
      "|    |    └─ProjectReadout: 3-7               [-1, 196, 768]            1,180,416\n",
      "|    |    └─Transpose: 3-8                    [-1, 768, 196]            --\n",
      "|    └─Sequential: 2                          []                        --\n",
      "|    |    └─ProjectReadout: 3-9               [-1, 196, 768]            1,180,416\n",
      "|    |    └─Transpose: 3-10                   [-1, 768, 196]            --\n",
      "|    └─Sequential: 2                          []                        --\n",
      "|    |    └─Conv2d: 3-11                      [-1, 96, 14, 14]          73,824\n",
      "|    |    └─ConvTranspose2d: 3-12             [-1, 96, 56, 56]          147,552\n",
      "|    └─Sequential: 2                          []                        --\n",
      "|    |    └─Conv2d: 3-13                      [-1, 192, 14, 14]         147,648\n",
      "|    |    └─ConvTranspose2d: 3-14             [-1, 192, 28, 28]         147,648\n",
      "|    └─Sequential: 2                          []                        --\n",
      "|    |    └─Conv2d: 3-15                      [-1, 384, 14, 14]         295,296\n",
      "|    └─Sequential: 2                          []                        --\n",
      "|    |    └─Conv2d: 3-16                      [-1, 768, 14, 14]         590,592\n",
      "|    |    └─Conv2d: 3-17                      [-1, 768, 7, 7]           5,309,184\n",
      "├─Module: 1                                   []                        --\n",
      "|    └─Conv2d: 2-1                            [-1, 256, 56, 56]         221,184\n",
      "|    └─Conv2d: 2-2                            [-1, 256, 28, 28]         442,368\n",
      "|    └─Conv2d: 2-3                            [-1, 256, 14, 14]         884,736\n",
      "|    └─Conv2d: 2-4                            [-1, 256, 7, 7]           1,769,472\n",
      "|    └─FeatureFusionBlock_custom: 2-5         [-1, 256, 14, 14]         --\n",
      "|    |    └─ResidualConvUnit_custom: 3-18     [-1, 256, 7, 7]           1,180,672\n",
      "|    |    └─Conv2d: 3-19                      [-1, 256, 14, 14]         65,792\n",
      "|    └─FeatureFusionBlock_custom: 2-6         [-1, 256, 28, 28]         --\n",
      "|    |    └─ResidualConvUnit_custom: 3-20     [-1, 256, 14, 14]         1,180,672\n",
      "|    |    └─ResidualConvUnit_custom: 3-21     [-1, 256, 14, 14]         1,180,672\n",
      "|    |    └─Conv2d: 3-22                      [-1, 256, 28, 28]         65,792\n",
      "|    └─FeatureFusionBlock_custom: 2-7         [-1, 256, 56, 56]         --\n",
      "|    |    └─ResidualConvUnit_custom: 3-23     [-1, 256, 28, 28]         1,180,672\n",
      "|    |    └─ResidualConvUnit_custom: 3-24     [-1, 256, 28, 28]         1,180,672\n",
      "|    |    └─Conv2d: 3-25                      [-1, 256, 56, 56]         65,792\n",
      "|    └─FeatureFusionBlock_custom: 2-8         [-1, 256, 112, 112]       --\n",
      "|    |    └─ResidualConvUnit_custom: 3-26     [-1, 256, 56, 56]         1,180,672\n",
      "|    |    └─ResidualConvUnit_custom: 3-27     [-1, 256, 56, 56]         1,180,672\n",
      "|    |    └─Conv2d: 3-28                      [-1, 256, 112, 112]       65,792\n",
      "|    └─Sequential: 2-9                        [-1, 12, 224, 224]        --\n",
      "|    |    └─Conv2d: 3-29                      [-1, 256, 112, 112]       589,824\n",
      "|    |    └─BatchNorm2d: 3-30                 [-1, 256, 112, 112]       512\n",
      "|    |    └─ReLU: 3-31                        [-1, 256, 112, 112]       --\n",
      "|    |    └─Dropout: 3-32                     [-1, 256, 112, 112]       --\n",
      "|    |    └─Conv2d: 3-33                      [-1, 12, 112, 112]        3,084\n",
      "|    |    └─Interpolate: 3-34                 [-1, 12, 224, 224]        --\n",
      "===============================================================================================\n",
      "Total params: 23,873,996\n",
      "Trainable params: 23,873,996\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 20.88\n",
      "===============================================================================================\n",
      "Input size (MB): 2.87\n",
      "Forward/backward pass size (MB): 163.71\n",
      "Params size (MB): 91.07\n",
      "Estimated Total Size (MB): 257.65\n",
      "===============================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "├─Module: 1                                   []                        --\n",
       "|    └─VisionTransformer: 2                   []                        --\n",
       "|    |    └─Dropout: 3-1                      [-1, 197, 768]            --\n",
       "|    |    └─LayerNorm: 3-2                    [-1, 197, 768]            1,536\n",
       "|    └─Sequential: 2                          []                        --\n",
       "|    |    └─ProjectReadout: 3-3               [-1, 196, 768]            1,180,416\n",
       "|    |    └─Transpose: 3-4                    [-1, 768, 196]            --\n",
       "|    └─Sequential: 2                          []                        --\n",
       "|    |    └─ProjectReadout: 3-5               [-1, 196, 768]            1,180,416\n",
       "|    |    └─Transpose: 3-6                    [-1, 768, 196]            --\n",
       "|    └─Sequential: 2                          []                        --\n",
       "|    |    └─ProjectReadout: 3-7               [-1, 196, 768]            1,180,416\n",
       "|    |    └─Transpose: 3-8                    [-1, 768, 196]            --\n",
       "|    └─Sequential: 2                          []                        --\n",
       "|    |    └─ProjectReadout: 3-9               [-1, 196, 768]            1,180,416\n",
       "|    |    └─Transpose: 3-10                   [-1, 768, 196]            --\n",
       "|    └─Sequential: 2                          []                        --\n",
       "|    |    └─Conv2d: 3-11                      [-1, 96, 14, 14]          73,824\n",
       "|    |    └─ConvTranspose2d: 3-12             [-1, 96, 56, 56]          147,552\n",
       "|    └─Sequential: 2                          []                        --\n",
       "|    |    └─Conv2d: 3-13                      [-1, 192, 14, 14]         147,648\n",
       "|    |    └─ConvTranspose2d: 3-14             [-1, 192, 28, 28]         147,648\n",
       "|    └─Sequential: 2                          []                        --\n",
       "|    |    └─Conv2d: 3-15                      [-1, 384, 14, 14]         295,296\n",
       "|    └─Sequential: 2                          []                        --\n",
       "|    |    └─Conv2d: 3-16                      [-1, 768, 14, 14]         590,592\n",
       "|    |    └─Conv2d: 3-17                      [-1, 768, 7, 7]           5,309,184\n",
       "├─Module: 1                                   []                        --\n",
       "|    └─Conv2d: 2-1                            [-1, 256, 56, 56]         221,184\n",
       "|    └─Conv2d: 2-2                            [-1, 256, 28, 28]         442,368\n",
       "|    └─Conv2d: 2-3                            [-1, 256, 14, 14]         884,736\n",
       "|    └─Conv2d: 2-4                            [-1, 256, 7, 7]           1,769,472\n",
       "|    └─FeatureFusionBlock_custom: 2-5         [-1, 256, 14, 14]         --\n",
       "|    |    └─ResidualConvUnit_custom: 3-18     [-1, 256, 7, 7]           1,180,672\n",
       "|    |    └─Conv2d: 3-19                      [-1, 256, 14, 14]         65,792\n",
       "|    └─FeatureFusionBlock_custom: 2-6         [-1, 256, 28, 28]         --\n",
       "|    |    └─ResidualConvUnit_custom: 3-20     [-1, 256, 14, 14]         1,180,672\n",
       "|    |    └─ResidualConvUnit_custom: 3-21     [-1, 256, 14, 14]         1,180,672\n",
       "|    |    └─Conv2d: 3-22                      [-1, 256, 28, 28]         65,792\n",
       "|    └─FeatureFusionBlock_custom: 2-7         [-1, 256, 56, 56]         --\n",
       "|    |    └─ResidualConvUnit_custom: 3-23     [-1, 256, 28, 28]         1,180,672\n",
       "|    |    └─ResidualConvUnit_custom: 3-24     [-1, 256, 28, 28]         1,180,672\n",
       "|    |    └─Conv2d: 3-25                      [-1, 256, 56, 56]         65,792\n",
       "|    └─FeatureFusionBlock_custom: 2-8         [-1, 256, 112, 112]       --\n",
       "|    |    └─ResidualConvUnit_custom: 3-26     [-1, 256, 56, 56]         1,180,672\n",
       "|    |    └─ResidualConvUnit_custom: 3-27     [-1, 256, 56, 56]         1,180,672\n",
       "|    |    └─Conv2d: 3-28                      [-1, 256, 112, 112]       65,792\n",
       "|    └─Sequential: 2-9                        [-1, 12, 224, 224]        --\n",
       "|    |    └─Conv2d: 3-29                      [-1, 256, 112, 112]       589,824\n",
       "|    |    └─BatchNorm2d: 3-30                 [-1, 256, 112, 112]       512\n",
       "|    |    └─ReLU: 3-31                        [-1, 256, 112, 112]       --\n",
       "|    |    └─Dropout: 3-32                     [-1, 256, 112, 112]       --\n",
       "|    |    └─Conv2d: 3-33                      [-1, 12, 112, 112]        3,084\n",
       "|    |    └─Interpolate: 3-34                 [-1, 12, 224, 224]        --\n",
       "===============================================================================================\n",
       "Total params: 23,873,996\n",
       "Trainable params: 23,873,996\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 20.88\n",
       "===============================================================================================\n",
       "Input size (MB): 2.87\n",
       "Forward/backward pass size (MB): 163.71\n",
       "Params size (MB): 91.07\n",
       "Estimated Total Size (MB): 257.65\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9648477a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): HybridEmbed(\n",
       "    (backbone): ResNetV2(\n",
       "      (stem): Sequential(\n",
       "        (conv): StdConv2dSame(3, 64, kernel_size=(7, 7), stride=(2, 2), bias=False)\n",
       "        (norm): GroupNormAct(\n",
       "          32, 64, eps=1e-05, affine=True\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (pool): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
       "      )\n",
       "      (stages): Sequential(\n",
       "        (0): ResNetStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (downsample): DownsampleConv(\n",
       "                (conv): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm): GroupNormAct(\n",
       "                  32, 256, eps=1e-05, affine=True\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (conv1): StdConv2dSame(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 64, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 64, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): StdConv2dSame(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 64, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 64, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): StdConv2dSame(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 64, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 64, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResNetStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (downsample): DownsampleConv(\n",
       "                (conv): StdConv2dSame(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (norm): GroupNormAct(\n",
       "                  32, 512, eps=1e-05, affine=True\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (conv1): StdConv2dSame(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 128, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 128, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 512, eps=1e-05, affine=True\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): StdConv2dSame(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 128, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 128, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 512, eps=1e-05, affine=True\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): StdConv2dSame(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 128, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 128, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 512, eps=1e-05, affine=True\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): StdConv2dSame(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 128, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 128, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 512, eps=1e-05, affine=True\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ResNetStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (downsample): DownsampleConv(\n",
       "                (conv): StdConv2dSame(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (norm): GroupNormAct(\n",
       "                  32, 1024, eps=1e-05, affine=True\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (conv1): StdConv2dSame(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 1024, eps=1e-05, affine=True\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 1024, eps=1e-05, affine=True\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 1024, eps=1e-05, affine=True\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 1024, eps=1e-05, affine=True\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (4): Bottleneck(\n",
       "              (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 1024, eps=1e-05, affine=True\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (5): Bottleneck(\n",
       "              (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 1024, eps=1e-05, affine=True\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (6): Bottleneck(\n",
       "              (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 1024, eps=1e-05, affine=True\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (7): Bottleneck(\n",
       "              (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 1024, eps=1e-05, affine=True\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (8): Bottleneck(\n",
       "              (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 1024, eps=1e-05, affine=True\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): Identity()\n",
       "      (head): ClassifierHead(\n",
       "        (global_pool): SelectAdaptivePool2d (pool_type=, flatten=Identity())\n",
       "        (fc): Identity()\n",
       "        (flatten): Identity()\n",
       "      )\n",
       "    )\n",
       "    (proj): Conv2d(1024, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (pre_logits): Identity()\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pretrained.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfe14945",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('weights/segmentation/dpt_hybrid-ade20k-53898607.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce0e9f",
   "metadata": {},
   "source": [
    "model.load_state_dict(checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nazir_env",
   "language": "python",
   "name": "nazir_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
